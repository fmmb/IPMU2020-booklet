\documentclass[../booklet.tex]{subfiles}

\begin{document}

\subsection[Why Spiking Neural Networks Are Efficient: A Theorem. {\it Michael Beer, Julio Urenda, Olga Kosheleva and Vladik Kreinovich}]{Why Spiking Neural Networks Are Efficient: A Theorem}
\index[authors]{Beer, Michael} \index[authors]{Urenda,  Julio} \index[authors]{Kosheleva,  Olga} \index[authors]{Kreinovich, Vladik}

\begin{center}
  {\it Michael Beer, Julio Urenda, Olga Kosheleva and Vladik Kreinovich}
\end{center}
%\begin{minipage}{1\textwidth}
%\end{minipage}

\vskip 0.8cm


Current artificial neural networks are very successful in many
machine learning applications, but in some cases they still lag
behind human abilities. To improve their performance, a natural
idea is to simulate features of biological neurons which are not
yet implemented in machine learning. One of such features is the
fact that in biological neural networks, signals are represented by
a train of spikes. Researchers have tried adding this spikiness to
machine learning and indeed got very good results, especially when
processing time series (and, more generally, spatio-temporal data).
In this paper, we provide a possible theoretical explanation for this
empirical success.

\keywords{Spiking neural networks  \and Shift-invariance \and Scale-invariance.}



\end{document}
