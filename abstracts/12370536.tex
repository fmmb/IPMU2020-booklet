\documentclass[../booklet.tex]{subfiles}

\begin{document}

\subsection[Evaluation of uncertainty quantification in deep learning. {\it Niclas Ståhl, Göran Falkman, Alexander Karlsson and Gunnar Mathiason}]{Evaluation of uncertainty quantification in deep learning}
\index[authors]{Ståhl, Niclas} \index[authors]{Falkman,  Göran} \index[authors]{Karlsson,  Alexander} \index[authors]{Mathiason, Gunnar}

\begin{center}
  {\it Niclas Ståhl, Göran Falkman, Alexander Karlsson and Gunnar Mathiason}
\end{center}
%\begin{minipage}{1\textwidth}
%\end{minipage}

\vskip 0.8cm


  Artificial intelligence (AI) is nowadays included into an increasing number of critical systems.
  Inclusion of AI in such systems may, however, pose a risk, since it is, still, infeasible to build AI systems that know how to function well in situations that differ greatly from what the AI has seen before.
  Therefore, it is crucial that future AI systems have the ability to not only function well in known domains, but also understand and show when they are uncertain when facing something unknown.
  In this paper, we evaluate four different methods that have been proposed to correctly quantifying uncertainty when the AI model is faced with new samples.
  We investigate the behaviour of these models when they are applied to samples far from what these models have seen before, and if they correctly attribute those samples with high uncertainty.
  We also examine if incorrectly classified samples are attributed with an higher uncertainty than correctly classified samples.
  The major finding from this simple experiment is, surprisingly, that the evaluated methods capture the uncertainty differently and the correlation between the quantified uncertainty of the models is low.
  This inconsistency is something that needs to be further understood and solved before AI can be used in critical applications in a trustworthy and safe manner.



\end{document}
